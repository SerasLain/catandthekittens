{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия CAT_collocates.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SerasLain/catandthekittens/blob/master/CAT_collocates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skHpTN2tTKm2",
        "colab_type": "text"
      },
      "source": [
        "# Поиск коллокаций в CAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zXpdYp5vdOT",
        "colab_type": "code",
        "outputId": "8e3c8a2f-170d-41e0-b31d-3784f5e75fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Connection to my Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Some useful paths\n",
        "cat_d = '/content/gdrive/My Drive/CAT/'\n",
        "cat_together = cat_d + 'Склеенные/'\n",
        "cat_cleaned = cat_d + 'Cleaned/'\n",
        "path = '/content/gdrive/My Drive/Новые conll по доменам/NewVers/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Hf6Uflu3nc",
        "colab_type": "text"
      },
      "source": [
        "## Препроцессинг нового корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RyDosXdUcs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjbX6HCAiohz",
        "colab_type": "code",
        "outputId": "3c466b98-2655-4bf2-87da-c376b51ef718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSLU4aYAb4BA",
        "colab_type": "code",
        "outputId": "c834b680-4f16-4eb0-f54f-7c498e364292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Connection to my Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NBYgWuAc1nc",
        "colab_type": "code",
        "outputId": "bad934de-b0d7-42d6-85b6-9fa97ea52db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Файл с целиковым корпусом проще записать из остальных файлов, чем препроцессить отдельно\n",
        "cat_d = '/content/gdrive/My Drive/CAT/'\n",
        "cat_together = cat_d + 'Склеенные/'\n",
        "files = os.listdir(cat_together)\n",
        "files.pop(0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CAT.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfPxFuadFGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From https://github.com/MariaFjodorowa/catandthekittens/blob/develop/CYBERsuffering/Preprocessing%20and%20modelling%20example.ipynb\n",
        "# by Alex Klimov\n",
        "#относительно хороший результат чистки корпуса\n",
        "stopwords = ['б', 'г', 'д', 'е', 'ё', 'ж', 'з', 'й', 'л', 'м', 'н', 'п', 'р', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'гг']\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "goal_d = cat_d + 'Cleaned/'\n",
        "if not os.path.exists(goal_d):\n",
        "  os.mkdir(goal_d)\n",
        "work_d = cat_together\n",
        "for i, file in enumerate(files):\n",
        "    text = os.path.abspath(cat_together + file)\n",
        "    with open(text, 'r', encoding = 'utf-8') as t:\n",
        "      txt = t.read()  \n",
        "    txt = txt.lower()\n",
        "    tokenized_text = tokenizer.tokenize(txt)\n",
        "    lst = []\n",
        "    for tryingtoclean in tokenized_text:\n",
        "        tryingtoclean = re.sub('(данные об авторе)(\\.)?.*', '', tryingtoclean, flags=re.S)\n",
        "        tryingtoclean = re.sub(r'[a-zA-z]', '', tryingtoclean)\n",
        "        tryingtoclean = re.sub(r'_', '', tryingtoclean)\n",
        "        tryingtoclean = re.sub(r'\\d', '', tryingtoclean)\n",
        "        tryingtoclean = re.sub(r'[ñâÿçàíšßãóìøþôõýæùúñî¹áîřðûɫɥɨêɜǎäiöüéɨïіïëèåòè]', '', tryingtoclean)\n",
        "        words = re.findall(r'\\w+', tryingtoclean)\n",
        "        clean = filter(lambda a: a not in stopwords, words)\n",
        "        x = ' '.join(clean).rstrip()\n",
        "        lst.append(x)\n",
        "    xxx = '. '.join(lst).rstrip() + '.'\n",
        "    dotpattern = re.compile(r'(\\. \\.)+')\n",
        "    xxx = re.sub(dotpattern, '', xxx)\n",
        "    spacepattern = re.compile(r'(  )+')\n",
        "    xxx = re.sub(spacepattern, '', xxx)\n",
        "    dotpattern2 = re.compile(r'( \\.)+')\n",
        "    xxx = re.sub(dotpattern2, '', xxx)\n",
        "# Отсюда убран небольшой кусок кода, поскольку он ошибочно срабатывал на строках типа \"однако он\", превращая их в \"однакн\"\n",
        "    goal_f = open(goal_d + file, 'w', encoding = 'utf-8')\n",
        "    goal_f.write(xxx)\n",
        "    goal_f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm88UqOAl9GH",
        "colab_type": "code",
        "outputId": "0ee671f9-4179-41d5-fb04-60fca35220a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Checking out\n",
        "with open(goal_d + 'Economics.txt') as f:\n",
        "  t = f.read()\n",
        "  \n",
        "print(t[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "введение в в российской экономике начался восстановительный рост. однако он остается слабым и неустойчивым при этом сохраняются угрозы новой рецессии статья подготовлена при финансовой поддержке рффи проект санкт петербургский государственный университет вестник санкт петербургского университета. вып. экономика шедших десятилетия россия пережила три циклических спада характерных для среднесрочных колебаний капиталистической экономики. при всех существенных отличиях свойственных этим кризисам в п\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yb-H6mmTKm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing\n",
        "\n",
        "\n",
        "#Склеиваем все тексты в один файл для каждого домена, а потом эти файлы в один общий для всего корпуса.\n",
        "\n",
        "def make_raws(root=cat_d):\n",
        "    failed = []\n",
        "    for d in os.listdir(root):\n",
        "        with open((os.path.join(root, d) + r'.txt'), 'a', encoding='utf-8') as domain:\n",
        "            for f in os.listdir(os.path.join(root, d)):\n",
        "                print(f)\n",
        "                with open(os.path.join(root, d, f), 'r', encoding='utf-8') as t:\n",
        "                    try:\n",
        "                        text = t.read()\n",
        "                        domain.write(text)\n",
        "                        domain.write('\\n')\n",
        "                    except UnicodeDecodeError:\n",
        "                        failed.append(os.path.join(root, d, f))\n",
        "    print(failed)\n",
        "    \n",
        "make_raws()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXRPvaTjTKnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_corpus(root='.'):\n",
        "    for root, dirs, files in os.walk(root):\n",
        "        for f in files:\n",
        "            with open(os.path.join(root, 'CAT.txt'), 'a', encoding='utf8') as corpora:\n",
        "                with open(os.path.join(root, f), 'r', encoding='utf8', errors='ignore') as f:\n",
        "                    domain = f.read()\n",
        "                corpora.write(domain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r763BqSqfJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_corpus(root=goal_d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdk8HgJ2u-r2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Извлечение коллокаций\n",
        "\n",
        "Версия для старого корпуса https://github.com/MariaFjodorowa/catandthekittens/tree/develop/collocations/collocation_frequencies\n",
        "by Anna Dmitrieva\n",
        "\n",
        "Код переработан для лучшей читаемости и оформлен по pep8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GREps0vzKxLY",
        "colab_type": "code",
        "outputId": "7ddd9e8a-8564-42d9-ac15-cfc7b2705eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "pip install conllu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-yHW4cy0gw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from conllu import parse, parse_tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UyPvrfHHw15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parser(filename):\n",
        "  \"\"\"\n",
        "  Yields a sentence from conllu tree with its tags\n",
        "  \n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  >>> for i in parser('/content/gdrive/My Drive/Новые conll по доменам/NewVers/CleanedPsyEdu.conllu'):\n",
        "      print(i)   \n",
        "  TokenList<Музыка, звучит, отовсюду, независимо, от, нашего, желания, или, нежелания, слушать, ее, .>\n",
        "  \"\"\"\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "  tree = parse(data)\n",
        "  for token in tree:\n",
        "    yield token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntNrt10xIj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = parser('/content/gdrive/My Drive/Новые conll по доменам/NewVers/CleanedPsyEdu.conllu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR37chZ3sk4U",
        "colab_type": "code",
        "outputId": "063ea14f-a6ad-4e8d-da3d-304d76c4d8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "os.listdir('/content/gdrive/My Drive/Новые conll по доменам/NewVers/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PsyEdu.xlsx',\n",
              " 'Domains_collocations',\n",
              " 'CleanedEco.conllu',\n",
              " 'CleanedLaw.conllu',\n",
              " 'CleanedLing.conllu',\n",
              " 'CleanedPsyEdu.conllu',\n",
              " 'CleanedSocioHist.conllu',\n",
              " 'CleanedCAT.conllu',\n",
              " 'Eco.conllu.xlsx',\n",
              " 'PsyEdu.csv',\n",
              " 'PsyEdu_uni.csv',\n",
              " 'PsyEdu_unigrams.csv',\n",
              " 'PsyEdu_bi.csv',\n",
              " 'PsyEdu_bi.csv_tscore.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYuoQ3fwOfu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Извлекаем слова\n",
        "def get_words(tree):\n",
        "  \"\"\"\n",
        "  tree - generator of sentences (TokenLists) from conllu tree\n",
        "  \n",
        "  words, list is a list of all tokens we need from the tree\n",
        "  size, int is a number of all words in the domain\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  for sentence in tree:\n",
        "    for token in sentence:\n",
        "      if token['form'] != '_' and token['upostag'] != '_' and token['upostag']!='NONLEX' and token['form'] not in r'[]\\/':\n",
        "        for wordform in token['form'].lower().split():\n",
        "          words.append((wordform, token['upostag']))\n",
        "  size = len(words)\n",
        "  return words, size # arr of tuples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6NnGgwC6ci6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, size = get_words(tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suJgD2dD6-YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kraa2Q3N7Atz",
        "colab_type": "code",
        "outputId": "906f8fd2-afea-4205-a589-b5fc2788d2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2084647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJO0UJ86I2lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "def get_ngrams(words, n):\n",
        "    # arr of tuples, int\n",
        "    \"\"\"\n",
        "    words, list is a list of all tokens we need from the tree\n",
        "    n, int is a number of words in n-gram, from 1 to 6\n",
        "    \n",
        "    ngrams, defaultdict is a dictionary of ngrams\n",
        "    \"\"\"\n",
        "    ngrams = defaultdict(list)\n",
        "    if n == 1:\n",
        "      #unigrams is just a dictionary of all tokens with their counting in the domain\n",
        "        for i in words:\n",
        "            if i[1] != 'SENT':\n",
        "              try:\n",
        "                  ngrams[i][0] += 1\n",
        "              except IndexError:\n",
        "                ngrams[i].append(1)\n",
        "              ngrams[i].append(i[1])\n",
        "    else:\n",
        "        for i in range(len(words) - n):\n",
        "            ngram = words[i:(i + n)]\n",
        "            for tup in ngram:\n",
        "                if tup[1]=='SENT':\n",
        "                    ngram.clear()\n",
        "                    break\n",
        "            if ngram:\n",
        "                word = ''\n",
        "                tag = ''\n",
        "                for tup in ngram:\n",
        "                  # хз, насколько осмысленно собирать их в строку с пробелами, а не в массив\n",
        "                  word = word + tup[0] + ' '\n",
        "                  tag = tag + tup[1] + ' '\n",
        "                try:\n",
        "                  ngrams[word[:-1]][0] += 1\n",
        "                except IndexError:\n",
        "                  ngrams[word[:-1]].append(1)\n",
        "                ngrams[word[:-1]].append(tag[:-1])\n",
        "    return ngrams #arr of tuples of two strings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl3KVUU75_4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = get_ngrams(words, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiuTjnWqLh_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_csv(ngrams, path):\n",
        "  with open(path, 'a', encoding='utf-8') as f:\n",
        "    for ngram in ngrams:\n",
        "      f.write('\\t'.join([ngram, str(ngrams[ngram][0]), ngrams[ngram][1]])+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4ZlZp-K29M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_csv(trigrams, path+'PsyEdu_tri.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwpKKclMC9uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARyTLjLxWEvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams = {}\n",
        "def read_ngrams(path, container):\n",
        "  \"\"\"\n",
        "  Load dictionary of ngrams from csv\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  path, str is a path to source csv\n",
        "  container, dictionary is an empty dict which will be filled with collocations\n",
        "  \"\"\"\n",
        "  \n",
        "  file = open(path, 'r', encoding='utf-8')\n",
        "  for line in file:\n",
        "    collocation, freq, tags = line.strip('\\n').split('\\t')\n",
        "    container[collocation] = [freq, tags]\n",
        "  return container"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9hFKcX7Vm-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Слово частота тэг\n",
        "unigrams = {}\n",
        "file = open(path+'PsyEdu_unigrams.csv','r', encoding='utf-8')\n",
        "for line in file:\n",
        "  print(line)\n",
        "  word_tag, freq = line.strip('\\n').split('\\t')\n",
        "  unigrams[word_tag] = [freq]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBew-EMBWsv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trigrams = get_ngrams(words, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNOGSbZMPmbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raw_freq(colloc_tup, size):\n",
        "  \"\"\"\n",
        "  colloc_tup, tuple is a tuple like ('word another_word', 3, 'Tag another_tag')\n",
        "  \n",
        "  rf, float is a raw_frequency\n",
        "  \n",
        "  \"\"\"\n",
        "  count = colloc_tup[2]\n",
        "  rf = int(count)/size\n",
        "  return rf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPX-x0raQ6s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def t_score(colloc_tup, unigrams, size):\n",
        "  \"\"\"\n",
        "  colloc_tup, tuple is a tuple like ('word another_word', 'Tag another_tag', 3)\n",
        "  unigrams, defaultdict is a dict of word:frequency in corpora, frequency is int\n",
        "  \n",
        "  score, float is a t-score metric\n",
        "  counted by formula from http://webground.su/data/lit/pivovarova_yagunova/\n",
        "  Izvlechenie_i_klassifikatsiya_terminoligicheskih_kollokatsyi.pdf\n",
        "  \"\"\"\n",
        "  collocation = colloc_tup[0].split(' ')\n",
        "  tags = colloc_tup[1].split(' ')\n",
        "  collocation_frequency = int(colloc_tup[2])\n",
        "  collocation_dict = {}\n",
        "  for i in range(len(collocation)):\n",
        "    collocation_dict[collocation[i]] = int(unigrams[(collocation[i], tags[i])][0])\n",
        "  score = (collocation_frequency - float(np.prod(list(collocation_dict.values()))))/math.sqrt(collocation_frequency)\n",
        "  return score\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "busyIyu5HexS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_dice(colloc_tup, unigrams):\n",
        "  \"\"\"\n",
        "  colloc_tup, tuple is a tuple like ('word another_word', 'Tag another_tag', 3)\n",
        "  unigrams, defaultdict is a dict of word:frequency in corpora, frequency is int\n",
        "  \n",
        "  score, float is a logDice metric, from https://www.fi.muni.cz/usr/sojka/download/raslan2008/13.pdf\n",
        "  \"\"\"\n",
        "  collocation = colloc_tup[0].split(' ')\n",
        "  tags = colloc_tup[1].split(' ')\n",
        "  collocation_frequency = int(colloc_tup[2])\n",
        "  collocation_dict = {}\n",
        "  for i in range(len(collocation)):\n",
        "    collocation_dict[collocation[i]] = int(unigrams[(collocation[i], tags[i])][0])\n",
        "  score = 14 + math.log2(2 * collocation_frequency / sum(collocation_dict.values()))\n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bI7qv0JLhZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Реализация likelyhood ratio (в процессе разработки)\n",
        "def L(k, n, x):\n",
        "  \"\"\"\n",
        "  k, n, x is floats or integers\n",
        "  B, float is a binomial coefficient\n",
        "  \"\"\"\n",
        "  B = math.pow(x, k) * math.pow((1 - x), (n - k))\n",
        "  return B\n",
        "\n",
        "def likelyhood_ratio(colloc_tup, unigrams, size):\n",
        "   \"\"\"\n",
        "  colloc_tup, tuple is a tuple like ('word another_word', 'Tag another_tag', 3)\n",
        "  unigrams, defaultdict is a dict of word:frequency in corpora, frequency is int\n",
        "  \n",
        "  score, float is a likelyhood ratio metric, from https://nlp.stanford.edu/fsnlp/promo/colloc.pdf Manning&Shutz\n",
        "  \"\"\"  \n",
        "  p = c2 / corpus_len\n",
        "  p1 = c12 / c1\n",
        "  p2 = (c2 - c12) / (corpus_len - c1)\n",
        "  lik_rat = -2*(math.log(L(c12,c1,p)) + math.log(L(c2 - c12, corpus_len - c1, p)) - math.log(L(c12, c1, p1)) - math.log(L(c2 - c12, corpus_len - c1, p2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHFQxJNdDtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bi = path+'PsyEdu_bi.csv'\n",
        "def by_row(collocation_csv):\n",
        "  \"\"\"\n",
        "  Takes a dataset with collocation, frequenvcies and tags to process it row by row and write results in the same way into another dataset\n",
        "  \"\"\"\n",
        "  \n",
        "  \"\"\"\n",
        "  collocation_csv, str is a path to the file with ngrams, counting and tags\n",
        "  \"\"\"\n",
        "  f =  open(collocation_csv, 'r', encoding='utf-8')\n",
        "  file = open(collocation_csv+'_tscore.csv', 'a', encoding='utf-8')\n",
        "  for line in f:\n",
        "    collocation, freq, tags = line.strip('\\n').split('\\t')\n",
        "    rf = raw_freq((collocation, tags, freq), size)\n",
        "    t_sc = t_score((collocation, tags, freq), unigrams, size)\n",
        "    logD = log_dice((collocation, tags, freq), unigrams)\n",
        "    file.write('\\t'.join([collocation, tags, str(freq), str(rf), str(t_sc), str(logD)]) + '\\n')\n",
        "  file.close()\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5gHoGM0lJNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "by_row(bi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdv7KD8FnP7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri = path+'PsyEdu_tri.csv'\n",
        "by_row(tri)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}